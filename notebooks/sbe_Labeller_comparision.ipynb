{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import olac\n",
    "from copy import deepcopy\n",
    "from IPython import display\n",
    "import functools \n",
    "import os\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n",
    "% matplotlib inline\n",
    "def get_grid():\n",
    "    gridpoints = np.linspace(-10, 10, 250)\n",
    "    grid = []\n",
    "    for y in gridpoints:\n",
    "        for x in gridpoints:\n",
    "            grid.append([x, y])\n",
    "    return np.array(grid)\n",
    "\n",
    "grid = get_grid()\n",
    "\n",
    "def scale_generator(original_generator):\n",
    "    @functools.wraps(original_generator)\n",
    "    def wrapper(x_min, x_max, dp0=0, dp1=1, *args, **kwargs):\n",
    "        or_output = original_generator(*args, **kwargs)\n",
    "        \n",
    "        x_shift = x_min\n",
    "\n",
    "        dp = np.array((dp0, dp1))\n",
    "        x_min -= x_shift\n",
    "        x_max -= x_shift\n",
    "        for point in or_output:\n",
    "            point[:2] -= x_shift\n",
    "            point[:2] = dp[0] + (point[:2] - x_min)*(dp[1] - dp[0])/(x_max - x_min)\n",
    "            yield point\n",
    "    return wrapper\n",
    "\n",
    "transformer = RBFSampler(n_components=100, gamma=0.075)\n",
    "transformer.fit_transform(grid)\n",
    "\n",
    "@scale_generator\n",
    "def new_generator(generator, *args, **kwargs):\n",
    "\n",
    "    gen = eval(generator)(*args, **kwargs)\n",
    "    return gen\n",
    "\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewPredictor(olac.pipeline.OnlinePredictor):\n",
    "    def __init__(self, verbose, transformer):\n",
    "        super().__init__()\n",
    "        self.model_hist = []\n",
    "        self.verbose = verbose\n",
    "        self.transformer = transformer\n",
    "        self.models = []\n",
    "        self.grid = self.get_grid()\n",
    "        \n",
    "    def transform(self, X_train):\n",
    "        if hasattr(self.transformer, 'transform'):\n",
    "            return self.transformer.transform(X_train)\n",
    "        else:\n",
    "            return X_train\n",
    "        \n",
    "        \n",
    "    def train_pipeline_model(self, pipeline):\n",
    "        points = pipeline.training_queue.get_all()\n",
    "\n",
    "        X_train = np.vstack([np.array(p.point) for p in points])\n",
    "        y_train = np.array([p.true_label for p in points])\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Predictor:\\t{len(points)} new points available, updating ...'\n",
    "            )\n",
    "\n",
    "        pipeline.model.partial_fit(self.transform(X_train), y_train, classes=[0, 1])\n",
    "        try:\n",
    "            mod = deepcopy(pipeline.model)\n",
    "            self.models.append(mod)\n",
    "            if hasattr(mod, 'coef_'):\n",
    "                self.model_hist.append((mod.coef_, mod.intercept_, mod.classes_, \n",
    "                                        X_train, y_train))\n",
    "            elif hasattr(mod, 'coefs_'):\n",
    "                self.model_hist.append((mod.coefs_, mod.intercepts_.copy(),\n",
    "                                        mod.classes_.copy(), mod.n_outputs_,\n",
    "                                        mod.n_layers_, mod.out_activation_,\n",
    "                                        mod._label_binarizer, X_train, y_train))\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def get_grid(self):\n",
    "        gridpoints = np.linspace(-10, 10, 250)\n",
    "        grid = []\n",
    "        for y in gridpoints:\n",
    "            for x in gridpoints:\n",
    "                grid.append([x, y])\n",
    "        return np.array(grid)\n",
    "    \n",
    "    def do_prediction(self, pipeline, x,):\n",
    "        \"\"\"\n",
    "        Make a prediction. If the model has not yet been fit (burn-in phase),\n",
    "        return NaNs.\n",
    "        \"\"\"\n",
    "        xt = self.transform([x])\n",
    "#         if self.verbose:\n",
    "#             print(f\"Making predictions for {len(x)} points\")\n",
    "        try:\n",
    "            y_pred = pipeline.model.predict(xt)\n",
    "\n",
    "            # certainty measure not standard across sklearn\n",
    "            # prefer predict_proba, else take decision_function\n",
    "            if hasattr(pipeline.model, 'predict_proba'):\n",
    "                prob = pipeline.model.predict_proba(xt)\n",
    "#                 self.models.append(pipeline.model.predict_proba(self.transform(self.grid))[:,0].reshape(250,250))\n",
    "            elif hasattr(pipeline.model, 'decision_function'):\n",
    "                prob = pipeline.model.decision_function(xt)\n",
    "#                 self.models.append(pipeline.model.decision_function(self.transform(self.grid))[:].reshape(250,250))\n",
    "            else:\n",
    "                prob = np.nan\n",
    "\n",
    "        except (sklearn.exceptions.NotFittedError, AssertionError):\n",
    "            # still burning in, return NaNs.\n",
    "            y_pred = np.nan\n",
    "            prob = np.nan\n",
    "\n",
    "        return y_pred, prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CertaintyLabeller(olac.pipeline.LabellerBase):\n",
    "    \"\"\"A simple labeller. Once the number of points in the\n",
    "    pipeline.labelling_queue reaches a certain threshold, all points are\n",
    "    retrieved. Points are then randomly labelled with a certain probability.\n",
    "\n",
    "    The total number of labels purchased is tracked using an internal\n",
    "    property. This could be used for budgeting.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold, prob, verbose=True):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        threshold: int\n",
    "            The minimum number of points to trigger a batch of labelling\n",
    "\n",
    "        prob: float (<= 1)\n",
    "            The probability with which each point will receive a label\n",
    "\n",
    "        verbose: bool\n",
    "            Whether to print output when labelling\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.prob = prob\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.labels_bought = 0\n",
    "\n",
    "    def buy_labels_condition(self, pipeline: olac.pipeline.Pipeline,):\n",
    "        \"\"\"Buy labels if the labelling_queue is longer than the threshold.\"\"\"\n",
    "        n = pipeline.labelling_queue.qsize()\n",
    "        if n > self.threshold:\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'Labeller:\\tThreshold met, {n} new '\n",
    "                    'points available in queue'\n",
    "                )\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def buy_labels(self, pipeline: olac.pipeline.Pipeline,):\n",
    "        \"\"\"Get all the points from the labelling queue and label them with\n",
    "        some probability. \"\"\"\n",
    "        labelled_points = []\n",
    "        unlabelled_points = []\n",
    "\n",
    "        points = pipeline.labelling_queue.get_all()\n",
    "        \n",
    "        # -- filter out the nans\n",
    "        list_points = [p.to_tuple() for p in points if type(p.to_tuple()[-2]) == np.ndarray]\n",
    "\n",
    "        if len(list_points) == 0:\n",
    "            list_points = [p.to_tuple() for p in points]\n",
    "        else:\n",
    "            list_points = sorted(list_points, key=lambda point: point[-2][0][1], reverse=True)\n",
    "        \n",
    "        try:\n",
    "            n = int(len(list_points)/2)\n",
    "            top = np.array(list_points)[:n,1]\n",
    "            \n",
    "#         top10 = np.array(top10)[:, 1]\n",
    "        \n",
    "            for point in points:\n",
    "                # self.prob percent chance of being labelled\n",
    "                if point.index in top:\n",
    "                    self.labels_bought += 1\n",
    "                    labelled_points.append(point)\n",
    "                else:\n",
    "                    unlabelled_points.append(point)\n",
    "        except IndexError:\n",
    "            print(\"No points added\")\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f'Labeller:\\tLabelled {len(labelled_points)} new points')\n",
    "        \n",
    "        return labelled_points, unlabelled_points    \n",
    "\n",
    "class GreedyLabeller(olac.pipeline.ThresholdLabeller):\n",
    "    def buy_labels(self, pipeline):\n",
    "        \n",
    "        def get_prob(point):\n",
    "            p = point.prob\n",
    "            try:\n",
    "                return p.flatten()[-1]\n",
    "            except:\n",
    "                return p\n",
    "        \n",
    "        points = pipeline.labelling_queue.get_all()\n",
    "        points = list(sorted(\n",
    "            points, key=get_prob, reverse=True\n",
    "        ))\n",
    "        \n",
    "        n_labs = int(np.round(self.prob*len(points)))\n",
    "        n_labs = max(n_labs, 1)\n",
    "        n_labs = min(n_labs, len(points))\n",
    "        \n",
    "        labelled_points = points[:n_labs]\n",
    "        unlabelled_points = points[n_labs:]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'Labeller:\\tLabelled {len(labelled_points)} new points')\n",
    "\n",
    "        return labelled_points, unlabelled_points\n",
    "    \n",
    "    \n",
    "class VeryGreedyLabeller(olac.pipeline.ThresholdLabeller):\n",
    "    def buy_labels(self, pipeline):\n",
    "        \n",
    "        def get_prob(point):\n",
    "            p = point.prob\n",
    "            try:\n",
    "                return p.flatten()[-1]\n",
    "            except:\n",
    "                return p\n",
    "        \n",
    "        labelled_points = []\n",
    "        unlabelled_points = []\n",
    "        \n",
    "        points = pipeline.labelling_queue.get_all()\n",
    "\n",
    "        for point in points:\n",
    "            prob = get_prob(point)\n",
    "            \n",
    "            if np.isnan(prob):\n",
    "                labelled_points.append(point)\n",
    "            elif prob >= 0.5:\n",
    "                labelled_points.append(point)\n",
    "            elif np.random.uniform() < self.prob:\n",
    "                labelled_points.append(point)\n",
    "            else:\n",
    "                unlabelled_points.append(point)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'Labeller:\\tLabelled {len(labelled_points)} new points')\n",
    "\n",
    "        return labelled_points, unlabelled_points\n",
    "    \n",
    "\n",
    "class UncertaintyLabeller(olac.pipeline.ThresholdLabeller):\n",
    "    def buy_labels(self, pipeline):\n",
    "        \n",
    "        prob = hasattr(pipeline.model, 'predidct_proba')\n",
    "        \n",
    "        def uncertainty(point):\n",
    "            try:\n",
    "                p = point.prob.flatten()[-1]\n",
    "            except:\n",
    "                p = point.prob\n",
    "                \n",
    "            if prob:\n",
    "                uncert = (p-0.5)**2\n",
    "            else:\n",
    "                uncert = p**2\n",
    "            \n",
    "            return uncert\n",
    "\n",
    "        points = pipeline.labelling_queue.get_all()\n",
    "        points = list(sorted(\n",
    "            points, key=uncertainty, reverse=False\n",
    "        ))\n",
    "        \n",
    "        n_labs = int(np.round(self.prob*len(points)))\n",
    "        n_labs = max(n_labs, 1)\n",
    "        n_labs = min(n_labs, len(points))\n",
    "        \n",
    "        labelled_points = points[:n_labs]\n",
    "        unlabelled_points = points[n_labs:]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f'Labeller:\\tLabelled {len(labelled_points)} new points')\n",
    "\n",
    "        return labelled_points, unlabelled_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the new labeller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = list(new_generator(generator='olac.data_generators.satellites', x_min=-10, x_max=10, n_points=3000))\n",
    "# transformer = RBFSampler(n_components=100, gamma=0.09)\n",
    "# # transformer = SkewedChi2Sampler(0.1, 100)\n",
    "# gridt = transformer.fit_transform(get_grid())\n",
    "# sns.set_context(\"notebook\")\n",
    "\n",
    "# pa_pipeline = olac.pipeline.Pipeline(\n",
    "#     olac.data_generators.delayed_generator(data, delay=0.05),\n",
    "#     MLPClassifier((64,32,), batch_size=1, learning_rate='adaptive', early_stopping=False, \n",
    "#                     alpha=0.0003, warm_start=False),\n",
    "#     NewPredictor(verbose=True, transformer=transformer),\n",
    "#     CertaintyLabeller(threshold=10, prob=0.5, verbose=True))\n",
    "    \n",
    "# pa_train_set, pa_test_set = pa_pipeline.run()\n",
    "# # plot_history(pa_pipeline, pa_train_set, save_gif=False, trans=transformer)\n",
    "# # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_history(pa_pipeline, pa_train_set, gen_name='satellites 1% fraud',\n",
    "#                     save_gif=False, trans=transformer, plot_cost=True)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter = Plotter(window=20)\n",
    "# plotter.plot_history(pa_pipeline, pa_train_set, gen_name='Satellites_test',\n",
    "#                      save_gif=False, trans=transformer, plot_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = list(new_generator(generator='olac.data_generators.cluster_generator', x_min=0, x_max=1000, n_points=3000))\n",
    "# transformer = RBFSampler(n_components=100, gamma=0.09)\n",
    "# # transformer = SkewedChi2Sampler(0.1, 100)\n",
    "# gridt = transformer.fit_transform(get_grid())\n",
    "# sns.set_context(\"notebook\")\n",
    "\n",
    "# cg_pipeline = olac.pipeline.Pipeline(\n",
    "#     olac.data_generators.delayed_generator(data, delay=0.05),\n",
    "#     MLPClassifier((64,32,), batch_size=1, learning_rate='adaptive', early_stopping=False, \n",
    "#                     alpha=0.0003, warm_start=False),\n",
    "#     NewPredictor(verbose=True, transformer=transformer),\n",
    "#     CertaintyLabeller(threshold=10, prob=0.5, verbose=True))\n",
    "    \n",
    "# cg_train_set, cg_test_set = cg_pipeline.run()\n",
    "# # plot_history(pa_pipeline, pa_train_set, save_gif=False, trans=transformer)\n",
    "# # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotter = olac.vis.Plotter(window=20, figsize=(15, 5))\n",
    "# plotter.plot_history(cg_pipeline, cg_train_set, gen_name='popping_clusters',\n",
    "#                      save_gif=True, trans=transformer, plot_cost=True, budget=100, gain_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = RBFSampler(n_components=100, gamma=0.075)\n",
    "# transformer.fit_transform(get_grid());\n",
    "model = MLPClassifier((64,32,), batch_size=1, learning_rate='adaptive', early_stopping=False, \n",
    "                    alpha=0.0003, warm_start=False)\n",
    "labellers = [olac.pipeline.ThresholdLabeller(threshold=10, prob=.05, verbose=False),\n",
    "            ]\n",
    "data_generators = [new_generator(generator='olac.data_generators.satellites', x_min=-10, x_max=10, n_points=3000,\n",
    "                                 contamination=0.3, satellite_radius=6),\n",
    "                   new_generator(generator='olac.data_generators.roving_balls', x_min=-10, x_max=10, steps=3000),\n",
    "                   new_generator(generator='olac.data_generators.cluster_generator', x_min=0, x_max=1000, n_points=3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThresholdLabeller\n",
      "new_generator\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f043fd49204e6283f4cc8a2672ae6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------\n",
      "ThresholdLabeller\n",
      "new_generator\n"
     ]
    }
   ],
   "source": [
    "pipelines = []\n",
    "\n",
    "for gen in data_generators:\n",
    "    generator = list(gen)\n",
    "    for labeller in labellers:\n",
    "        mod_name = str(labeller.__class__).split('.')[-1].split(\"'\")[0]\n",
    "        print(mod_name)\n",
    "        trans = transformer\n",
    "        print(gen.__name__)\n",
    "#         model = clone(mod)\n",
    "        pipeline = olac.pipeline.Pipeline(data_generator=olac.data_generators.delayed_generator(generator, delay=0.15),\n",
    "                                          model=clone(model),\n",
    "                                          predictor=NewPredictor(verbose=False, transformer=trans),\n",
    "                                          labeller=labeller)\n",
    "        train_set, test_set = tqdm_notebook(pipeline.run())\n",
    "        pipelines.append((pipeline, train_set, test_set))\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- run same with john's labellers\n",
    "\n",
    "labeller_kwargs = {\n",
    "    'threshold': 10,\n",
    "    'prob': 0.05,\n",
    "    'verbose': False,\n",
    "}\n",
    "\n",
    "# transformer = RBFSampler(n_components=100, gamma=0.075)\n",
    "# transformer.fit_transform(get_grid());\n",
    "model = MLPClassifier((64,32,), batch_size=1, learning_rate='adaptive', early_stopping=False, \n",
    "                    alpha=0.0003, warm_start=False)\n",
    "\n",
    "labellers_john = [UncertaintyLabeller(**labeller_kwargs),\n",
    "                  VeryGreedyLabeller(**labeller_kwargs),\n",
    "                  GreedyLabeller(**labeller_kwargs)]\n",
    "\n",
    "data_generators = [new_generator(generator='olac.data_generators.satellites', x_min=-10, x_max=10, n_points=3000,\n",
    "                                 contamination=0.3, satellite_radius=6),\n",
    "                   new_generator(generator='olac.data_generators.roving_balls', x_min=-10, x_max=10, steps=3000),\n",
    "                   new_generator(generator='olac.data_generators.cluster_generator', x_min=0, x_max=1000, n_points=3000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines_john = []\n",
    "\n",
    "for gen in data_generators:\n",
    "    generator = list(gen)\n",
    "    for labeller in labellers_john:\n",
    "        mod_name = str(labeller.__class__).split('.')[-1].split(\"'\")[0]\n",
    "        print(mod_name)\n",
    "        trans = transformer\n",
    "        print(gen.__name__)\n",
    "#         model = clone(mod)\n",
    "        pipeline = olac.pipeline.Pipeline(data_generator=olac.data_generators.delayed_generator(generator, delay=0.15),\n",
    "                                          model=clone(model),\n",
    "                                          predictor=NewPredictor(verbose=False, transformer=trans),\n",
    "                                          labeller=labeller)\n",
    "        train_set, test_set = tqdm_notebook(pipeline.run())\n",
    "        pipelines_john.append((pipeline, train_set, test_set))\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# gen_names = ['Satellites', 'Satellites','Satellite',\n",
    "#                  'Roving Balls','Roving Balls', 'Roving Balls',\n",
    "#                  'Clusters', 'Clusters','Clusters',]\n",
    "\n",
    "# lab_names =['Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "#             'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "#             'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',]\n",
    "\n",
    "# for i, (pipeline, train_set, test_set) in enumerate(pipelines_john):       \n",
    "#     plotter = olac.vis.Plotter(window=20, figsize=(20, 9))\n",
    "#     plotter.plot_history(pipeline, test_set, save_gif=True, gen_name=gen_names[i]+str(i),\n",
    "#                          budget=100, gain_factor=2, trans=transformer, plot_cost=True, plot_step=1,\n",
    "#                         sup_title=f\"DataSet: {gen_names[i]} -- {lab_names[i]}\")\n",
    "#     plt.show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decider = olac.cost_of_label.simple_decider\n",
    "labeller_kwargs = {\n",
    "    'threshold': 10,\n",
    "    'decider': decider,\n",
    "    'decider_kwargs':{\"prob\": .05}\n",
    "}\n",
    "bram_labeller = olac.pipeline.NaieveLabeller(**labeller_kwargs)\n",
    "\n",
    "# transformer = RBFSampler(n_components=100, gamma=0.075)\n",
    "# transformer.fit_transform(get_grid());\n",
    "transformer\n",
    "model = MLPClassifier((64,32,), batch_size=1, learning_rate='adaptive', early_stopping=False, \n",
    "                    alpha=0.0003, warm_start=False)\n",
    "\n",
    "data_generators = [new_generator(generator='olac.data_generators.satellites', x_min=-10, x_max=10, n_points=3000,\n",
    "                                 contamination=0.3, satellite_radius=6),\n",
    "                   new_generator(generator='olac.data_generators.roving_balls', x_min=-10, x_max=10, steps=3000),\n",
    "                   new_generator(generator='olac.data_generators.cluster_generator', x_min=0, x_max=1000, n_points=3000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipelines_bram = []\n",
    "\n",
    "for gen in tqdm_notebook(data_generators):\n",
    "    generator = list(gen)\n",
    "\n",
    "    mod_name = str(bram_labeller.__class__).split('.')[-1].split(\"'\")[0]\n",
    "    print(mod_name)\n",
    "    trans = transformer\n",
    "    print(gen.__name__)\n",
    "#         model = clone(mod)\n",
    "    pipeline = olac.pipeline.Pipeline(data_generator=olac.data_generators.delayed_generator(generator, delay=0.15),\n",
    "                                      model=clone(model),\n",
    "                                      predictor=NewPredictor(verbose=False, transformer=trans),\n",
    "                                      labeller=bram_labeller)\n",
    "    train_set, test_set = pipeline.run()\n",
    "    pipelines_bram.append((pipeline, train_set, test_set))\n",
    "    print('-'*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Online vs offline\n",
    "- naive vs olac >> random >> little less random >> lac (backup = something in the middle)\n",
    "- make table of results \n",
    "    - acc mean + var\n",
    "    - f1\n",
    "    - precision\n",
    "    - recall\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for P in [pipelines, pipelines_john, pipelines_bram]:\n",
    "\n",
    "\n",
    "    gen_names = ['Satellites', 'Satellites','Satellite',\n",
    "                     'Roving Balls','Roving Balls', 'Roving Balls',\n",
    "                     'Clusters', 'Clusters','Clusters',]\n",
    "    if P is pipelines_bram:\n",
    "        gen_names = gen_names[[0,3,6]]\n",
    "        lab_names = ['Naive Labeller', 'Naive Labeller', 'Naive Labeller', ]\n",
    "    elif P is pipelines:\n",
    "        gen_names = gen_names[[0,3,6]]\n",
    "        lab_names = [ 'ThresholdLabeller', 'ThresholdLabller', 'ThresholdLabeller']\n",
    "    else:\n",
    "        lab_names =['Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "                'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "                'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',]\n",
    "\n",
    "\n",
    "    for i, (pipeline, train_set, test_set) in enumerate(pipelines_john):       \n",
    "        plotter = olac.vis.Plotter(window=20, figsize=(20, 9))\n",
    "        plotter.plot_history(pipeline, test_set, save_gif=True, gen_name=gen_names[i]+str(i),\n",
    "                             budget=100, gain_factor=2, trans=transformer, plot_cost=True, plot_step=2,\n",
    "                            sup_title=f\"DataSet: {gen_names[i]} -- {lab_names[i]}\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_names = ['Satellites',# 'Satellites','Satellite',\n",
    "#                  'Roving Balls',#'Roving Balls', 'Roving Balls',\n",
    "#                  'Clusters',]# 'Clusters','Clusters',]\n",
    "\n",
    "# # lab_names =['Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "# #             'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',\n",
    "# #             'Uncertainty Labeller', 'Very Greedy Labeller', 'Greedy Labeller',]\n",
    "\n",
    "# lab_names = ['Naive Labeller', 'Naive Labeller', 'Naive Labeller', ]\n",
    "# # lab_names = ['CertaintyLabeller', 'ThresholdLabeller', 'CertaintyLabeller',\n",
    "# #              'ThresholdLabller', 'CertaintyLabeller', 'ThresholdLabeller']\n",
    "             \n",
    "\n",
    "# for i, (pipeline, train_set, test_set) in enumerate(pipelines_bram):       \n",
    "#     plotter = olac.vis.Plotter(window=20, figsize=(20, 9))\n",
    "#     plotter.plot_history(pipeline, test_set, save_gif=True, gen_name=gen_names[i]+str(i),\n",
    "#                          budget=100, gain_factor=2, trans=transformer, plot_cost=True, plot_step=2,\n",
    "#                         sup_title=f\"DataSet: {gen_names[i]} -- {lab_names[i]}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_table(pipelines, which='test', gen_names=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pipelines: list\n",
    "        constains the pipeline objects\n",
    "    which: string\n",
    "        test, train or both\n",
    "        \"\"\"\n",
    "    \n",
    "    for j, run in enumerate(pipelines):\n",
    "        df_test = olac.utils.queue_point_list_to_df(run[2])\n",
    "        df_train = olac.utils.queue_point_list_to_df(run[1])\n",
    "\n",
    "        df_test.dropna(inplace=True); df_train.dropna(inplace=True)\n",
    "        \n",
    "        if which == 'test':\n",
    "            df = df_test.copy()\n",
    "        elif which == 'train':\n",
    "            df = df_train.copy()\n",
    "        elif which == 'both':\n",
    "            df = df_test.append(df_train)\n",
    "            df.sort_index(inplace=True)\n",
    "        else:\n",
    "            raise(\"Which can only be 'test', 'train' or 'both'\")\n",
    "\n",
    "        df_split = np.array_split(df.values, round(df.shape[0]/50))\n",
    "\n",
    "        acc = []\n",
    "        prec = []\n",
    "        recall = []\n",
    "        f1 = []\n",
    "        TN = []\n",
    "        FP = []\n",
    "        FN = []\n",
    "        TP = []\n",
    "\n",
    "        for i, arr in enumerate(df_split):\n",
    "            try:\n",
    "                tn, fp, fn, tp = sklearn.metrics.confusion_matrix(np.vstack(arr[:,-1]).ravel(),\n",
    "                                                                  np.vstack(arr[:,-4]).ravel(),\n",
    "                                                                  labels=[0,1]).ravel()\n",
    "            except ValueError:\n",
    "                return arr, i, j\n",
    "            TN.append(tn)\n",
    "            FP.append(fp)\n",
    "            FN.append(fn)\n",
    "            TP.append(tp)\n",
    "            acc.append((tp+tn)/(tn+fp+fn+tp))\n",
    "            prec.append(tp/(tp+fp))\n",
    "            recall.append((tp)/(tp+fn))\n",
    "            f1.append(sklearn.metrics.f1_score(np.vstack(arr[:,-1]).ravel(),\n",
    "                                               np.vstack(arr[:,-4]).ravel(),\n",
    "                                               labels=[0,1]))\n",
    "\n",
    "        run[0].describe(output=False)\n",
    "        new_df = pd.DataFrame(np.array([acc, prec, recall, f1, TP, FP, TN, FN]).T,\n",
    "                              columns=['acc','prec','recall','f1', 'tp','fp','tn','fn'])\n",
    "        new_df.fillna('-')\n",
    "        new_df['labeller'] = run[0].labeller_name\n",
    "        if gen_names is None:\n",
    "            new_df['dataset'] = run[0].data_generator_name\n",
    "        else:\n",
    "            new_df['dataset'] = gen_names[j]\n",
    "        new_df['model'] = run[0].model_name\n",
    "        \n",
    "        try:\n",
    "            df_final = df_final.append(new_df)\n",
    "        except NameError:\n",
    "            df_final = new_df\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipelines = get_metrics_table(pipelines[:6], which='test', gen_names = ['satellites', 'roving_balls',\n",
    "                 'clusters'])\n",
    "df_john_pipelines = get_metrics_table(pipelines_john, which='test',  gen_names = ['satellites', 'satellites', 'satellites', \n",
    "                                                     'roving_balls','roving_balls', 'roving_balls',\n",
    "                                                     'clusters', 'clusters','clusters',])\n",
    "df_bram_pipelines = get_metrics_table(pipelines_bram, which='test',  gen_names=['satellites', 'roving_balls',\n",
    "                 'clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for df in [df_pipelines.fillna(0), df_bram_pipelines.fillna(0), df_john_pipelines.fillna(0)]:\n",
    "    try:\n",
    "        del(df_results)\n",
    "    except NameError:\n",
    "        pass\n",
    "    mean_cols = ['labeller','dataset','model','acc','prec','recall','f1']\n",
    "    sum_cols = ['labeller','dataset','model','tp','fp','tn','fn']\n",
    "    \n",
    "    df_mean = df[mean_cols].groupby(['labeller','dataset','model']).mean().reset_index()\n",
    "    df_var = df[mean_cols].groupby(['labeller','dataset','model']).var().reset_index()\n",
    "    \n",
    "    df_sum = df[sum_cols].groupby(['labeller','dataset','model']).sum().reset_index()\n",
    "    \n",
    "    df_merge = df_mean.merge(df_var,\n",
    "                             left_on = ['labeller','dataset','model'],\n",
    "                             right_on = ['labeller','dataset','model'],\n",
    "                             suffixes=('', '_var'))\n",
    "\n",
    "    df_merge = df_merge.merge(df_sum,\n",
    "                         left_on = ['labeller','dataset','model'],\n",
    "                         right_on = ['labeller','dataset','model'],)\n",
    "    \n",
    "    df_merge = df_merge[['labeller','dataset','acc', 'acc_var',\n",
    "                         'f1', 'f1_var', 'prec', 'prec_var', 'recall', 'recall_var','tp','fp','tn','fn']]\n",
    "#     df_merge = df_merge.merge(df, left_on = ['labeller','dataset','model'],\n",
    "#                              right_on = ['labeller','dataset','model'],how='left')\n",
    "    \n",
    "    try:\n",
    "        df_results = df_results.append(df_merge)\n",
    "    except NameError:\n",
    "        df_results = df_merge\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pipelines = get_metrics_table(pipelines[:6], which='train', gen_names = ['satellites', 'satellites', \n",
    "                 'roving_balls','roving_balls',\n",
    "                 'clusters', 'clusters',])\n",
    "df_john_pipelines = get_metrics_table(pipelines_john, which='train',  gen_names = ['satellites', 'satellites', 'satellites', \n",
    "                                                     'roving_balls','roving_balls', 'roving_balls',\n",
    "                                                     'clusters', 'clusters','clusters',])\n",
    "df_bram_pipelines = get_metrics_table(pipelines_bram, which='train',  gen_names=['satellites','roving_balls','clusters'])\n",
    "\n",
    "\n",
    "for df in [df_pipelines.fillna(0), df_bram_pipelines.fillna(0), df_john_pipelines.fillna(0)]:\n",
    "    try:\n",
    "        del(df_results2)\n",
    "    except NameError:\n",
    "        pass\n",
    "    mean_cols = ['labeller','dataset','model','acc','prec','recall','f1']\n",
    "    sum_cols = ['labeller','dataset','model','tp','fp','tn','fn']\n",
    "    \n",
    "    df_mean = df[mean_cols].groupby(['labeller','dataset','model']).mean().reset_index()\n",
    "    df_var = df[mean_cols].groupby(['labeller','dataset','model']).var().reset_index()\n",
    "    \n",
    "    df_sum = df[sum_cols].groupby(['labeller','dataset','model']).sum().reset_index()\n",
    "    \n",
    "    df_merge = df_mean.merge(df_var,\n",
    "                             left_on = ['labeller','dataset','model'],\n",
    "                             right_on = ['labeller','dataset','model'],\n",
    "                             suffixes=('', '_var'))\n",
    "\n",
    "    df_merge = df_merge.merge(df_sum,\n",
    "                         left_on = ['labeller','dataset','model'],\n",
    "                         right_on = ['labeller','dataset','model'],)\n",
    "    \n",
    "    df_merge = df_merge[['labeller','dataset','acc', 'acc_var',\n",
    "                         'f1', 'f1_var', 'prec', 'prec_var', 'recall', 'recall_var','tp','fp','tn','fn']]\n",
    "#     df_merge = df_merge.merge(df, left_on = ['labeller','dataset','model'],\n",
    "#                              right_on = ['labeller','dataset','model'],how='left')\n",
    "    \n",
    "    try:\n",
    "        df_results2 = df_results2.append(df_merge)\n",
    "    except NameError:\n",
    "        df_results2 = df_merge\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST -  Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.groupby('dataset').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.groupby('labeller').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - bought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results2.groupby('dataset').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results2.groupby('labeller').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
